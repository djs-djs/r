---

title: "Data Analytics in R - Assignment 19"
author: "Don Smith"
output: github_document

---

# Part 1: Code

### 1. Read in data.
```{r}

library(caret)
library(tidyverse)

# import dataset
df <- readRDS("C:\\Users\\Owner\\Documents\\GitHub\\r\\class assignments\\Data Analytics in R\\Assignment 19\\data\\mod6HE_logit.rds")

head(df)
summary(df)

```

### 2. Initial loading of data, packages, and functions.
```{r}

# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative) 
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive) 
  neg_pred_value <- true_negative/(true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy), 
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}

```

### 3. Prepare data.
```{r}

# Not for the model (for use later)
logit1 <- df %>% 
  ungroup() %>% 
  select(store, week, high_med_rev, high_med_gp, high_med_gpm) 

# For use in the model
logit2 <- df %>% 
  ungroup() %>% 
  select(high_med_units, 
         size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per,
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Check that "positive" is last for the `my_confusion_matrix` to work 
contrasts(factor(logit2$high_med_units))

```

### 4. Partition the data into testing and training datasets.
```{r}

#install.packages('caret') (don't install twice)
library(caret)
set.seed(77) 
partition <- caret::createDataPartition(y=logit2$high_med_units, p=.75, list=FALSE)
data_train <- logit2[partition, ]
data_test <- logit2[-partition, ]

```

### 5. Train the multivariate model - these are the instructions part of machine learning.
```{r}

model_train <- glm(high_med_units ~ ., family=binomial, data=data_train)
summary(model_train)

```

### 6. Predict the response variable.
```{r}

predict_test <- predict(model_train, newdata=data_test, type='response')

```

### 7. Form table to look at the accuracy of the model.
```{r}

table2 <- table(predict_test>.5, data_test$high_med_units) #prediction on left and truth on top
my_confusion_matrix(table2)

```

### 8. Put the data back together for future use.
```{r}

# Put the prediction back into the test data
data_test$prediction <- predict_test

# Create a variable that shows if the prediction was correct 
# (We have to do the classification--in `round(prediction)`--since logistic regression gives us a probability)
data_test <- data_test %>% mutate(correct_prediction = if_else(round(prediction) == high_med_units, 'correct', 'WRONG!'))

# Add back the original data
temp1 <- logit1[-partition, ]
full_test <- bind_cols(temp1, data_test)

# For viewing in class
full_test <- full_test %>% select(store, week, high_med_units, prediction, correct_prediction, size, region, promo_units_per, salty_units_per) 

slice_sample(full_test, n=10)

```

# Part 2: Questions

#### 1.a (0.5 points) What feature/variable has the most negative statistically significant coefficient on the trained model summary?

###### The most negative statistically significant coefficient in the trained model summary is promo_units_per.

#### 1.b (1 point) Does selling a higher proportion of alternative beverages increase, decrease, or neither increase nor decrease the chance of having above median units sold? How do you know this?

###### It increases the chance of having above median units sold. We know this because the coefficient for altbev_units_per is 7.415e+00, which is above "1", defined by median units sold. Because p value is low, it is statistically significant.

#### 1.c (1 point) Does selling a higher proportion of velocity B units increase, decrease, or neither increase nor decrease the chance of having above median units sold? How do you know this?

###### It does not increase the chance of having above median units sold. Since the p value is 0.13526 (p < 0.05 means it is statistically significant), this is not statistically significant. A larger (insignificant) p-value suggests that changes in the predictor are not associated with changes in the response.

#### 1.d (0.5 points) Examine the accuracy of the predictions on the test data by answering whether there are more true positives or more true negatives?

###### There are more true positives (975) than true negatives (904).

#### 1.e (1 point) If stores are sorted by the `store` feature in an ascending manner (lowest number first), which is the first store in the `full_test` dataset that has a “WRONG!” prediction?

```{r}

head(full_test %>% select(store, correct_prediction), n = 13L)

```

###### The first store that has a “WRONG!” prediction is Store 186 (row 13).

#### 2. (1 point) In the model training step, which data—training or testing—do we use and why (that is, explain why we split the data into training and testing subsets)?

###### A training set is partitioned from a data set in order to build a machine learning model. From that same data set, a test set is partitioned to validate the model. Data included in the training set is excluded from the test set. You want to "train" the model on the training set, but then to be sure it is accurate, then run it on the test set to ensure you're not over- or underfitting the model.

#### 3. (1 point) The feature `region` has changed in the summary of the trained model. Further, only three regions show up in the summary of the model. The reasoning for this is that the `glm()` function automatically recognizes that `region` is a categorical variable (specifically a factor in R). This is discussed in our Coursera content. Thus, the `glm()` function has created “dummy variables” for the levels of `region`. Which level of the variable is not present here but rather accounted for in the intercept term?

###### Coefficients tell us the effect of a particular independent variable on the dependent variable. The glm() function determined that region was a categorical variable, and automatically created dummy variables for each level of category. 

###### We now have three categorical variables: regionWEST, regionQUEBEC and regionATLANTIC. However, regionONTARIO is missing. This is because the glm() has included regionONTARIO in the intercept, which is done to avoid over-specifying the model.

###### Therefore, the ONTARIO region does not have an associated dummy variable and is accounted for in the intercept.

#### 4. (1 point) Interpret the confusion matrix using the test / holdout data. Specifically, which of the four measures, Sensitivity, Specificity, Precision, or Negative Predictive Value has the highest value? Write a sentence that translates this value into words. That is, say something that starts like this: “this means this model is good at predicting...”.

###### Sensitivity has the highest value, at 0.7653. This is equal to the amount of true positives the model correctly predicted. This means this model is good at predicting correct positive values.


#### 5. (1 point) Interpret the confusion matrix. Specifically, which of the four measures, Sensitivity, Specificity, Precision, or Negative Predictive Value has the lowest value? Write a sentence that translates this value into words. That is say something that starts like this: “this means this model is not as good at predicting…”.

###### Specificity has the lowest  value, at 0.7273. This is equal to the amount of true negatives the model correctly predicted. This means this model is good at predicting correct negative values.


#### 6. (2 points) Interpret the confusion matrix. In NANSE’s business setting, which of these measures does NANSE care about the most, sensitivity, specificity, precision, negative predictive value, or something else? Defend your answer in two or three sentences. There is no correct answer here, but you must successfully defend your answer to get credit.

###### I believe that NANSE would care more about sensitivity because they want to make sure the model is accurately predicting outcomes. While the other categories tell you important facts about the effectiveness of the model, sensitivity tells you the amount of true positives the model correctly predicts. From a business standpoint, effectiveness and accuracy are the most important elements.
