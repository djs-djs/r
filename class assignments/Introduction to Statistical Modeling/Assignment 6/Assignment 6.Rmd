---

title: "Introduction to Statistical Modeling - Assignment 6"
author: "Don Smith"
output: github_document

---

## Question 13.2

##### **In this problem you, can simulate a simplified airport security system at a busy airport. Passengers arrive according to a Poisson distribution with λ1 = 5 per minute (i.e., mean interarrival rate μ1 = 0.2 minutes) to the ID/boarding-pass check queue, where there are several servers who each have exponential service time with mean rate μ2 = 0.75 minutes. [Hint: model them as one block that has more than one resource.] After that, the passengers are assigned to the shortest of the several personal-check queues, where they go through the personal scanner (time is uniformly distributed between 0.5 minutes and 1 minute).** 
##### **Use the Arena software (PC users) or Python with SimPy (PC or Mac users) to build a simulation of the system, and then vary the number of ID/boarding-pass checkers and personal-check queues to determine how many are needed to keep average wait times below 15 minutes. [If you’re using SimPy, or if you have access to a non-student version of Arena, you can use λ1 = 50 to simulate a busier airport.]** 

> ##### I used SimPy to generate my simulation. See separate file entitled [Assignment  6 - Question 13.2](https://link-url-here.org).

## Question 14.1

##### **The breast cancer data set breast-cancer-wisconsin.data.txt from http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/ (description at http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29 ) has missing values.**
##### **1. Use the mean/mode imputation method to impute values for the missing data.**
##### **2. Use regression to impute values for the missing data.**
##### **3. Use regression with perturbation to impute values for the missing data.**
##### **4. (Optional) Compare the results and quality of classification models (e.g., SVM, KNN) build using**
##### **(1) the data sets from questions 1,2,3;**
##### **(2) the data that remains after data points with missing values are removed; and**
##### **(3) the data set when a binary variable is introduced to indicate missing values.**


```{r eval=TRUE}

# Loading all packages needed for analysis

library(kernlab)
library(kknn)
library(dplyr)
library(readr)
library(rmarkdown)
library(tinytex)
library(knitr)
library(NbClust)
library(outliers)
library(nortest)
library(mice)
library(Mcomp)
library(DAAG)
library(caret)
library(randomForest)
library(tree)
library(pROC)
library(ggplot2)
library(VIM)

cancer_data = read_delim("C:\\Users\\Owner\\Documents\\Github\\r\\class assignments\\Introduction to Statistical Modeling\\Assignment 6\\data\\breast-cancer-wisconsin.data.txt", 
                       delim = ',',
                       col_names = F, na = c('?')) %>% 
        as.data.frame() %>% 
        mutate(X11 = ifelse(X11 == 2, 'Benign', 'Malignant'))


head(cancer_data)

```

```{r eval=TRUE}

# Statistical summary of data

summary(cancer_data)

```


```{r eval=TRUE}

# Will use mice() function to locate column with missing data and render visually 

missing_print <- md.pattern(cancer_data[,-11])

print(missing_print)

# Looks like V7 has missing values.

```

```{r eval=TRUE}

# Plotting the missing data

plot_missing_values <- aggr(cancer_data, col = c('green', 'red'), numbers = TRUE, sortVars = TRUE)

```



```{r eval=TRUE}

# Using seed to generate reproducible results

set.seed(4233)

# Will use mice() function to do mean imputation

cancer_data_mean_impute <- mice(cancer_data, m = 5, meth = 'mean')

print(cancer_data_mean_impute$imp)

```

```{r eval=TRUE}

# Using seed to generate reproducible results

set.seed(4233)

# Will impute missing data points using mice() function. Using 'norm.predict', or the linear regression, predicted values method for this

cancer_data_regression_impute <- mice(cancer_data, m = 5, meth = 'norm.predict')

print(cancer_data_regression_impute$imp)

```

```{r eval=TRUE}

# Using seed to generate reproducible results

set.seed(4233)

# Will impute missing data points using mice() function. Using 'norm.nob', to do perturbation imputation

cancer_data_pertubation_impute <- mice(cancer_data, m = 5, meth = 'norm.nob')

print(cancer_data_pertubation_impute$imp)

```

```{r eval=TRUE}

# Converting implicit missing values into explicit missing values with the complete() function for all three imputations

cancer_mean_impute <- complete(cancer_data_mean_impute)

cancer_regression_impute <- complete(cancer_data_regression_impute)

cancer_pertubation_impute <- complete(cancer_data_pertubation_impute)

```

```{r eval=TRUE}

# Will create list containing datasets, and use random forest method to compare cross validation accuracy of each model

cancer_data_sets = list(cancer_data, cancer_mean_impute, cancer_regression_impute, cancer_pertubation_impute)

# Created list to contain accuracy outputs after iteration

cancer_final_val_output = list(no_imputation = NULL, mean_imputation = NULL, regression_imputation = NULL,
                    pertubation_imputation = NULL)

```

```{r eval=TRUE}

# Using training method, will iterate loop to fit each model via the random forest method, then print accuracy percentages for each and load into "list" cancer_final_val_output" list above

for (i in seq_along(cancer_data_sets)) {
        
        df = cancer_data_sets[[i]]
        
        # Using seed to generate reproducible results

        set.seed(4233)
        
        # Classification results
        
        in_td <- createDataPartition(df$X11, p = .75, list = FALSE)
        
        # Splitting data for training and test datasets
        
        train <- df[in_td , ] %>% na.omit()
        test <- df[-in_td ,] %>% na.omit()
        
        # Fitting model
        cancer_model_fit <- train(
                X11 ~ ., 
                method = 'rf',
                data = train, 
                metric = 'Accuracy',
                trControl = trainControl(
                        method = 'cv', 
                        number = 10
                )
                )
        
        cancer_final_val_output[[i]] = cancer_model_fit
        
}

```

```{r eval=TRUE}

# Print and view the results of each model and compare cross validation accuracy of each output

no_imp <- cancer_final_val_output$no_imputation$finalModel
mean_imp <- cancer_final_val_output$mean_imputation$finalModel
regress_imp <- cancer_final_val_output$regression_imputation$finalModel
pert_imp <- cancer_final_val_output$pertubation_imputation$finalModel

print(no_imp)
print(mean_imp)
print(regress_imp)
print(pert_imp)

```

## Question 15.1

##### **Describe a situation or problem from your job, everyday life, current events, etc., for which optimization would be appropriate. What data would you need?** 

> ##### At Fitbit, optimization can be used to determine how many customer support advocates we need at a given time during the day. For example, studies have shown that most people contact support between 11am and 1pm, which is when most people are on their lunch breaks, and between 4:30pm and 7pm when most people are getting off work. We can use optimization to determine the exact budget and the number of advocates we should have available relative to call volume at those particular times during the day. The data we would need is daily, weekly and monthly call/chat/email contact volume, along with customer representative schedules and salaries.
